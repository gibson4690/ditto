{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSgSb9vUtCyX",
        "outputId": "75293fc8-f8e0-44d4-b25c-2b85c98be661"
      },
      "source": [
        "# instead of cloning repo, mount J:\\projects\\2023-08-26 ditto\\ditto\n",
        "\n",
        "# clone the repo\n",
        "#!git clone https://github.com/megagonlabs/ditto\n",
        "\n",
        "\n",
        "%cd ditto/\n",
        "\n",
        "# install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# if the above command does not work:\n",
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 15.67 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/ditto\n",
            "Collecting gensim==3.8.1 (from -r requirements.txt (line 1))\n",
            "  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.19.2 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.19.2.zip (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_rD_PhUMkAz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_hfO6D_uLby",
        "outputId": "2bf4b90c-5904-4639-b148-05ef0878e511"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir --no-build-isolation ./\n",
        "%cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 11240, done.\u001b[K\n",
            "remote: Counting objects: 100% (3417/3417), done.\u001b[K\n",
            "remote: Compressing objects: 100% (335/335), done.\u001b[K\n",
            "remote: Total 11240 (delta 3204), reused 3108 (delta 3082), pack-reused 7823\u001b[K\n",
            "Receiving objects: 100% (11240/11240), 15.29 MiB | 18.06 MiB/s, done.\n",
            "Resolving deltas: 100% (7923/7923), done.\n",
            "/content/apex\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Processing /content/apex\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-c9ar7r13/apex.egg-info\n",
            "  writing /tmp/pip-modern-metadata-c9ar7r13/apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-c9ar7r13/apex.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-c9ar7r13/apex.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-c9ar7r13/apex.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-c9ar7r13/apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-c9ar7r13/apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-c9ar7r13/apex.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-c9ar7r13/apex-0.1.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/dist-packages (from apex==0.1) (23.1)\n",
            "Building wheels for collected packages: apex\n",
            "  Running command Building wheel for apex (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/_ucc_util.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/test\n",
            "  copying apex/contrib/test/__init__.py -> build/lib/apex/contrib/test\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/__init__.py -> build/lib/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib/apex/contrib/cudnn_gbn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/_transducer_ref.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/group_norm.py -> build/lib/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/__init__.py -> build/lib/apex/contrib/group_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/__init__.py -> build/lib/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib/apex/contrib/test/optimizers\n",
            "  creating build/lib/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/__init__.py -> build/lib/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib/apex/contrib/test/xentropy\n",
            "  creating build/lib/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/__init__.py -> build/lib/apex/contrib/test/focal_loss\n",
            "  creating build/lib/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/__init__.py -> build/lib/apex/contrib/test/layer_norm\n",
            "  creating build/lib/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/__init__.py -> build/lib/apex/contrib/test/clip_grad\n",
            "  creating build/lib/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib/apex/contrib/test/cudnn_gbn\n",
            "  creating build/lib/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/__init__.py -> build/lib/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib/apex/contrib/test/transducer\n",
            "  creating build/lib/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/test_fmha.py -> build/lib/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/__init__.py -> build/lib/apex/contrib/test/fmha\n",
            "  creating build/lib/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib/apex/contrib/test/index_mul_2d\n",
            "  creating build/lib/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/__init__.py -> build/lib/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib/apex/contrib/test/peer_memory\n",
            "  creating build/lib/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/__init__.py -> build/lib/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib/apex/contrib/test/group_norm\n",
            "  creating build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/__init__.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  creating build/lib/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/__init__.py -> build/lib/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib/apex/contrib/test/bottleneck\n",
            "  creating build/lib/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib/apex/contrib/test/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing requirements to apex.egg-info/requires.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-5yoirkhg/.tmp-b7ykffg4/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/batch_norm.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/group_norm/group_norm.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/channel_swap.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/test/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/test_bottleneck_module.py'\n",
            "  adding 'apex/contrib/test/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/test/clip_grad/test_clip_grad.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py'\n",
            "  adding 'apex/contrib/test/fmha/__init__.py'\n",
            "  adding 'apex/contrib/test/fmha/test_fmha.py'\n",
            "  adding 'apex/contrib/test/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/test/focal_loss/test_focal_loss.py'\n",
            "  adding 'apex/contrib/test/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/group_norm/test_group_norm.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/test_index_mul_2d.py'\n",
            "  adding 'apex/contrib/test/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/layer_norm/test_fast_layer_norm.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_mha_fused_softmax.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_dist_adam.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/test/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py'\n",
            "  adding 'apex/contrib/test/transducer/__init__.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_joint.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_loss.py'\n",
            "  adding 'apex/contrib/test/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/test/xentropy/test_label_smoothing.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/_transducer_ref.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/_ucc_util.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=371405 sha256=02bf82242dc21fa2c5d8a61b792e1db76a4b62a45846b6d88b4727c61c0d6cce\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q0jur_hx/wheels/83/7e/8e/84a535a8280d2de7a77a3fed6710fa86220721b5045c017df9\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKSZwBG_uyzV",
        "outputId": "b661cb5d-38e4-4828-a0a4-5d22ad18447c"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.5/125.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.29.35-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.31.35 (from awscli)\n",
            "  Downloading botocore-1.31.35-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docutils<0.17,>=0.10 (from awscli)\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0 (from awscli)\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (6.0.1)\n",
            "Collecting colorama<0.4.5,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.31.35->awscli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.31.35->awscli) (2.8.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.31.35->awscli) (1.16.0)\n",
            "Installing collected packages: urllib3, rsa, jmespath, docutils, colorama, botocore, s3transfer, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "Successfully installed awscli-1.29.35 botocore-1.31.35 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.2 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G07K5mE6JPF",
        "outputId": "fcdbffa2-3f67-4fdc-b626-8e26c1ffb4ad"
      },
      "source": [
        "%cd ditto"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines"
      ],
      "metadata": {
        "id": "fO49D4BeGlxb",
        "outputId": "3e718bbc-83c0-4775-be8f-d5d857080353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare datasets"
      ],
      "metadata": {
        "id": "vCMGQ1hPeBp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uas6W3khe31K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = \"/content/ditto/data/people/train.csv\"\n",
        "dest_path = \"/content/ditto/data/people/train.txt\""
      ],
      "metadata": {
        "id": "YW1QXEQneAvc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUR_AHMdqbi1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataset(source_path,dest_path,label_column):\n",
        "  dataset = pd.read_csv(source_path)\n",
        "  entity_a_columns = [row for row in dataset if row.startswith(\"a_\")]\n",
        "  entity_b_columns = [row for row in dataset if row.startswith(\"b_\")]\n",
        "\n",
        "  text = \"\"\n",
        "\n",
        "  for index, row in dataset.iterrows():\n",
        "    for c in entity_a_columns:\n",
        "      text += \"COL \"\n",
        "      text += str(c)[2:] #remove prefix a_\n",
        "      text += \" VAL \"\n",
        "      text += str(row[c])\n",
        "      text += \" \"\n",
        "    text += \"\\t\"\n",
        "    for c in entity_b_columns:\n",
        "      text += \"COL \"\n",
        "      text += str(c)[2:] #remove prefix b_\n",
        "      text += \" VAL \"\n",
        "      text += str(row[c])\n",
        "      text += \" \"\n",
        "    text += \"\\t\"\n",
        "    text += str(row[label_column])\n",
        "    text += \"\\n\"\n",
        "\n",
        "  #export output\n",
        "  text_file = open(dest_path, \"w\")\n",
        "  n = text_file.write(text)\n",
        "  text_file.close()\n"
      ],
      "metadata": {
        "id": "Q4I4TqwpjNuD"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_dataset(source_path = \"/content/ditto/data/people/train.csv\",\n",
        "             dest_path = \"/content/ditto/data/people/train.txt\",\n",
        "             label_column = \"label\")"
      ],
      "metadata": {
        "id": "HH04qJakfT0y"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_dataset(source_path = \"/content/ditto/data/people/valid.csv\",\n",
        "             dest_path = \"/content/ditto/data/people/valid.txt\",\n",
        "             label_column = \"label\")"
      ],
      "metadata": {
        "id": "kRiLf71SpOo4"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_dataset(source_path = \"/content/ditto/data/people/test.csv\",\n",
        "             dest_path = \"/content/ditto/data/people/test.txt\",\n",
        "             label_column = \"label\")"
      ],
      "metadata": {
        "id": "CSdSqk5DeA8T"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tune model with custom data"
      ],
      "metadata": {
        "id": "hdZeo7FSkPH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task custom_people \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 64 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 50 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da drop_col #\\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUFkZ3d9eBDE",
        "outputId": "3f5f0305-633c-4945-896d-099e1c22a45b"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 07:56:45.893098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.59833163022995\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 1: dev_f1=0.4210526315789474, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5582384467124939\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.4210526315789474, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6442548632621765\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.4210526315789474, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6547437906265259\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.4444444444444445, f1=0.5714285714285715, best_f1=0.5714285714285715\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.49320319294929504\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4444444444444445, f1=0.5714285714285715, best_f1=0.5714285714285715\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5917547345161438\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4444444444444445, f1=0.5714285714285715, best_f1=0.5714285714285715\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4848926365375519\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4444444444444445, f1=0.6666666666666666, best_f1=0.5714285714285715\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4002734422683716\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.4516129032258065, f1=0.0, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5158264636993408\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.4210526315789474, f1=0.5, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5235702991485596\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.4444444444444445, f1=0.6666666666666666, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.39660075306892395\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5333333333333333, f1=0.0, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.49265727400779724\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.823529411764706, f1=0.0, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38859590888023376\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.888888888888889, f1=0.0, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.40233153104782104\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6666666666666666, f1=0.8, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31631115078926086\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5454545454545454, f1=0.0, best_f1=0.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4006798565387726\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 16: dev_f1=0.9411764705882353, f1=0.8, best_f1=0.8\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31449905037879944\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 17: dev_f1=0.9411764705882353, f1=0.8, best_f1=0.8\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2705327272415161\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 18: dev_f1=0.761904761904762, f1=0.6666666666666666, best_f1=0.8\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25046318769454956\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 19: dev_f1=1.0, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.33671295642852783\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 20: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28428220748901367\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 21: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23580004274845123\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 22: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23903439939022064\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 23: dev_f1=0.888888888888889, f1=0.8, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24100059270858765\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 24: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32767605781555176\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 25: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1765562891960144\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 26: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23098871111869812\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 27: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1819230169057846\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 28: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1745523065328598\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 29: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18747921288013458\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 30: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24976705014705658\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 31: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21215352416038513\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 32: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19467125833034515\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 33: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17170913517475128\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 34: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2940644919872284\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 35: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1863011121749878\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 36: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2119683474302292\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 37: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2007739245891571\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 38: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18318352103233337\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 39: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.216343954205513\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 40: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22082126140594482\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 41: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19122198224067688\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 42: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.234183207154274\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 43: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23867373168468475\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 44: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16606947779655457\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 45: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18675939738750458\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 46: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1710686981678009\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 47: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27242109179496765\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 48: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1811138242483139\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 49: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24017126858234406\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 50: dev_f1=0.888888888888889, f1=0.6666666666666666, best_f1=0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task custom_people \\\n",
        "  --input_path /content/ditto/data/people/train.txt \\\n",
        "  --output_path output/people_output_train.txt \\\n",
        "  --lm roberta \\\n",
        "  --max_len 64 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnjfO4vF_Jq4",
        "outputId": "c0e3b290-2d5f-4d8a-bf7b-e968bc6d3f48"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 07:58:35.843631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "30it [00:00, 36168.19it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.888888888888889\n",
            "real_f1 = 0.888888888888889\n",
            "43it [00:00, 127010.61it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task custom_people \\\n",
        "  --input_path /content/ditto/data/people/valid.txt \\\n",
        "  --output_path output/people_output_valid.txt \\\n",
        "  --lm roberta \\\n",
        "  --max_len 64 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35h5Y4HCfuEX",
        "outputId": "d2ec7b3c-21a9-4588-a70d-74086169f7db"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 07:58:53.472218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "30it [00:00, 57561.35it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.888888888888889\n",
            "real_f1 = 0.888888888888889\n",
            "30it [00:00, 125452.76it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task custom_people \\\n",
        "  --input_path /content/ditto/data/people/test.txt \\\n",
        "  --output_path output/people_output_test.txt \\\n",
        "  --lm roberta \\\n",
        "  --max_len 64 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhlyZmNGzYU9",
        "outputId": "56da5473-c24f-45ef-f756-a049930dc98e"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 07:59:15.943714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "30it [00:00, 58908.76it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.888888888888889\n",
            "real_f1 = 0.888888888888889\n",
            "6it [00:00, 37900.34it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u9AezpXNg2Np"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format Output as CSV"
      ],
      "metadata": {
        "id": "MuKpgKKr7rbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_output_as_csv(source_path, raw_results_path, dest_path):\n",
        "  df = pd.read_csv(source_path)\n",
        "  df_results = pd.read_json(raw_results_path,lines=True)\n",
        "\n",
        "  df[\"match\"] = df_results[\"match\"]\n",
        "  df[\"match_confidence\"] = df_results[\"match_confidence\"]\n",
        "\n",
        "  df.to_csv(dest_path)\n",
        ""
      ],
      "metadata": {
        "id": "yDwT1MIY-Yxe"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_output_as_csv(source_path = \"/content/ditto/data/people/train.csv\",\n",
        "  raw_results_path = \"/content/ditto/output/people_output_train.txt\",\n",
        "  dest_path = \"/content/ditto/output/people_output_train.csv\")"
      ],
      "metadata": {
        "id": "QDIecE0P-wP5"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_output_as_csv(source_path = \"/content/ditto/data/people/valid.csv\",\n",
        "  raw_results_path = \"/content/ditto/output/people_output_valid.txt\",\n",
        "  dest_path = \"/content/ditto/output/people_output_valid.csv\")"
      ],
      "metadata": {
        "id": "nHfilGV-icNY"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_output_as_csv(source_path = \"/content/ditto/data/people/test.csv\",\n",
        "  raw_results_path = \"/content/ditto/output/people_output_test.txt\",\n",
        "  dest_path = \"/content/ditto/output/people_output_test.csv\")"
      ],
      "metadata": {
        "id": "nj_Q0RKrhSEr"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COReJWpS8ekO"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_e4G7nUd8enO"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkUd1Epd8eqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPZUJARUt_7U",
        "outputId": "af1275e2-4346-4c44-e434-0943f650682f"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 128 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 20 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --da drop_col\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-26 18:20:42.317312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 3.41MB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 12.1MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 13.0MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 33.6MB/s]\n",
            "Downloading model.safetensors: 100% 499M/499M [00:03<00:00, 133MB/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.5665071606636047\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4607182443141937\n",
            "step: 20, loss: 0.5191353559494019\n",
            "step: 30, loss: 0.3480338454246521\n",
            "step: 40, loss: 0.10023485869169235\n",
            "step: 50, loss: 0.11979644745588303\n",
            "step: 60, loss: 0.14031493663787842\n",
            "step: 70, loss: 0.21377235651016235\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.09316705167293549\n",
            "step: 90, loss: 0.1288658231496811\n",
            "step: 100, loss: 0.5108204483985901\n",
            "step: 110, loss: 0.011245671659708023\n",
            "step: 120, loss: 0.28027427196502686\n",
            "step: 130, loss: 0.12761446833610535\n",
            "step: 140, loss: 0.11386328190565109\n",
            "step: 150, loss: 0.004554091487079859\n",
            "step: 160, loss: 0.044515348970890045\n",
            "step: 170, loss: 0.04838710278272629\n",
            "step: 180, loss: 0.030326761305332184\n",
            "step: 190, loss: 0.018941057845950127\n",
            "step: 200, loss: 0.002435960341244936\n",
            "step: 210, loss: 0.0016790891531854868\n",
            "step: 220, loss: 0.008898601867258549\n",
            "step: 230, loss: 0.03694805130362511\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 1: dev_f1=0.9633740288568259, f1=0.9684684684684683, best_f1=0.9684684684684683\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017085397616028786\n",
            "step: 10, loss: 0.0388229675590992\n",
            "step: 20, loss: 0.04796819016337395\n",
            "step: 30, loss: 0.02731555886566639\n",
            "step: 40, loss: 0.03726615756750107\n",
            "step: 50, loss: 0.0009206398390233517\n",
            "step: 60, loss: 0.07953061908483505\n",
            "step: 70, loss: 0.04343416541814804\n",
            "step: 80, loss: 0.003046383848413825\n",
            "step: 90, loss: 0.0024172833655029535\n",
            "step: 100, loss: 0.012481199577450752\n",
            "step: 110, loss: 0.06575514376163483\n",
            "step: 120, loss: 0.00027719189529307187\n",
            "step: 130, loss: 0.015098283998668194\n",
            "step: 140, loss: 0.004243411123752594\n",
            "step: 150, loss: 0.047488704323768616\n",
            "step: 160, loss: 0.02101028524339199\n",
            "step: 170, loss: 0.013146950863301754\n",
            "step: 180, loss: 0.1753014475107193\n",
            "step: 190, loss: 0.026631144806742668\n",
            "step: 200, loss: 0.16098012030124664\n",
            "step: 210, loss: 0.06694097816944122\n",
            "step: 220, loss: 0.02230885811150074\n",
            "step: 230, loss: 0.035564322024583817\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9832026875699889, f1=0.9808773903262092, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0023623439483344555\n",
            "step: 10, loss: 0.00445949099957943\n",
            "step: 20, loss: 0.004459935240447521\n",
            "step: 30, loss: 0.0643749088048935\n",
            "step: 40, loss: 0.26880306005477905\n",
            "step: 50, loss: 0.00781260710209608\n",
            "step: 60, loss: 0.003431829158216715\n",
            "step: 70, loss: 0.014699325896799564\n",
            "step: 80, loss: 0.010318956337869167\n",
            "step: 90, loss: 0.0006611186545342207\n",
            "step: 100, loss: 0.042196668684482574\n",
            "step: 110, loss: 0.0005930361221544445\n",
            "step: 120, loss: 0.0014794335002079606\n",
            "step: 130, loss: 0.11420216411352158\n",
            "step: 140, loss: 0.007285694126039743\n",
            "step: 150, loss: 0.01790408045053482\n",
            "step: 160, loss: 0.003070475533604622\n",
            "step: 170, loss: 0.05128210037946701\n",
            "step: 180, loss: 0.011683809570968151\n",
            "step: 190, loss: 0.003732551820576191\n",
            "step: 200, loss: 0.09986135363578796\n",
            "step: 210, loss: 0.01463789027184248\n",
            "step: 220, loss: 0.03566262125968933\n",
            "step: 230, loss: 0.006995482370257378\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9854096520763187, f1=0.9843400447427293, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006526425946503878\n",
            "step: 10, loss: 0.009221955202519894\n",
            "step: 20, loss: 0.041700560599565506\n",
            "step: 30, loss: 0.012419816106557846\n",
            "step: 40, loss: 0.058506958186626434\n",
            "step: 50, loss: 0.0005515769007615745\n",
            "step: 60, loss: 0.004139892756938934\n",
            "step: 70, loss: 0.00812512170523405\n",
            "step: 80, loss: 0.00039353370084427297\n",
            "step: 90, loss: 0.0009223025990650058\n",
            "step: 100, loss: 0.07307511568069458\n",
            "step: 110, loss: 0.002472362481057644\n",
            "step: 120, loss: 0.022167202085256577\n",
            "step: 130, loss: 0.024377867579460144\n",
            "step: 140, loss: 0.04735853895545006\n",
            "step: 150, loss: 0.00028708644094876945\n",
            "step: 160, loss: 0.026896372437477112\n",
            "step: 170, loss: 0.02876165136694908\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ditto/ditto/train_ditto.py\", line 89, in <module>\n",
            "    train(train_dataset,\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 201, in train\n",
            "    train_step(train_iter, model, optimizer, scheduler, hp)\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 137, in train_step\n",
            "    scaled_loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "## Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10190428-ddd6-4e68-ccde-a7e3474dcccc"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the\n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task wdc_all_small \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 64 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 5 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da drop_col\n",
        "\n",
        "# Step 2: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task wdc_all_small \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --max_len 64 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-26 18:28:43.957804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.8009777665138245\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ditto/ditto/train_ditto.py\", line 89, in <module>\n",
            "    train(train_dataset,\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 201, in train\n",
            "    train_step(train_iter, model, optimizer, scheduler, hp)\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 123, in train_step\n",
            "    for i, batch in enumerate(train_iter):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/ditto/ditto/ditto_light/dataset.py\", line 80, in __getitem__\n",
            "    left, right = combined.split(' [SEP] ')\n",
            "ValueError: not enough values to unpack (expected 2, got 1)\n",
            "2023-08-26 18:28:57.338527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ditto/ditto/matcher.py\", line 314, in <module>\n",
            "    config, model = load_model(hp.task, hp.checkpoint_path,\n",
            "  File \"/content/ditto/ditto/matcher.py\", line 274, in load_model\n",
            "    raise ModelNotFoundError(checkpoint)\n",
            "ditto_light.exceptions.ModelNotFoundError: Model checkpoints/wdc_all_small/model.pt was not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os8fsSZxklGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5b8baa-acdb-48d6-9d9f-e27bca954bb2"
      },
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 64 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 10 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da drop_col #\\\n",
        "  #--balance\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-26 18:38:36.641267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.5504743456840515\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 1: dev_f1=0.358974358974359, f1=0.32558139534883723, best_f1=0.32558139534883723\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5762977004051208\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.38095238095238093, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26557838916778564\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6, f1=0.4210526315789473, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29831382632255554\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.35, f1=0.32558139534883723, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2194756418466568\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4057971014492754, f1=0.3888888888888889, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18310202658176422\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.3888888888888889, f1=0.37837837837837834, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17487062513828278\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.3888888888888889, f1=0.4444444444444445, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10131863504648209\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.3448275862068965, f1=0.2553191489361702, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023612340912222862\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.31746031746031744, f1=0.24, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012957234866917133\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.31746031746031744, f1=0.24, best_f1=0.4210526315789473\n",
            "2023-08-26 18:39:36.487549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 57891.96it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7857142857142857\n",
            "real_f1 = 0.7857142857142857\n",
            "91it [00:00, 98600.27it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_person.txt \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --max_len 64 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "OWCPmrDRG8VN",
        "outputId": "734baf4f-faef-404d-c63f-090adaed5e7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-26 18:57:41.968853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 61820.81it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7857142857142857\n",
            "real_f1 = 0.7857142857142857\n",
            "5it [00:00, 6734.59it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test data, copy to input folder\n",
        "\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Thomas Anderson COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Ong Ah Seng COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Ong Ah Seng Jeffrey COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Bulleit Bourbon Barrel Aged G'Knight COL Brew_Factory_Name VAL Oskar Blues Grill & Brew COL Style VAL American Amber / Red Ale COL ABV VAL 8.70 % \tCOL Beer_Name VAL Figure Eight Bourbon Barrel Aged Jumbo Love COL Brew_Factory_Name VAL Figure Eight Brewing COL Style VAL Barley Wine COL ABV VAL - \t0\n"
      ],
      "metadata": {
        "id": "4oCmbXZ0Jon8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}