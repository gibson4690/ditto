{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSgSb9vUtCyX",
        "outputId": "ebca4fec-2e75-4805-e3f3-4c29ab7c28ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 335, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 335 (delta 5), reused 26 (delta 2), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (335/335), 27.29 MiB | 12.17 MiB/s, done.\n",
            "Resolving deltas: 100% (147/147), done.\n",
            "/content/ditto\n",
            "Collecting gensim==3.8.1 (from -r requirements.txt (line 1))\n",
            "  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.23.5)\n",
            "Collecting regex==2019.12.20 (from -r requirements.txt (line 3))\n",
            "  Downloading regex-2019.12.20.tar.gz (679 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.8/679.8 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.10.1)\n",
            "Collecting sentencepiece==0.1.91 (from -r requirements.txt (line 5))\n",
            "  Downloading sentencepiece-0.1.91.tar.gz (500 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# clone the repo on colab\n",
        "!git clone https://github.com/gibson4690/ditto\n",
        "\n",
        "\n",
        "%cd ditto/\n",
        "\n",
        "# install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# if the above command does not work:\n",
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rD_PhUMkAz-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_hfO6D_uLby",
        "outputId": "e5e979e6-b8a2-4106-80c7-e337a71b54cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 11240, done.\u001b[K\n",
            "remote: Counting objects: 100% (3308/3308), done.\u001b[K\n",
            "remote: Compressing objects: 100% (327/327), done.\u001b[K\n",
            "remote: Total 11240 (delta 3095), reused 3007 (delta 2981), pack-reused 7932\u001b[K\n",
            "Receiving objects: 100% (11240/11240), 15.29 MiB | 12.37 MiB/s, done.\n",
            "Resolving deltas: 100% (7924/7924), done.\n",
            "/content/apex\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Processing /content/apex\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-ze078qea/apex.egg-info\n",
            "  writing /tmp/pip-modern-metadata-ze078qea/apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-ze078qea/apex.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-ze078qea/apex.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-ze078qea/apex.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-ze078qea/apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-ze078qea/apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-ze078qea/apex.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-ze078qea/apex-0.1.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.10/dist-packages (from apex==0.1) (23.1)\n",
            "Building wheels for collected packages: apex\n",
            "  Running command Building wheel for apex (pyproject.toml)\n",
            "\n",
            "\n",
            "  torch.__version__  = 2.0.1+cu118\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/_ucc_util.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/test\n",
            "  copying apex/contrib/test/__init__.py -> build/lib/apex/contrib/test\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/__init__.py -> build/lib/apex/contrib/cudnn_gbn\n",
            "  copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib/apex/contrib/cudnn_gbn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/_transducer_ref.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/group_norm.py -> build/lib/apex/contrib/group_norm\n",
            "  copying apex/contrib/group_norm/__init__.py -> build/lib/apex/contrib/group_norm\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/__init__.py -> build/lib/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib/apex/contrib/test/optimizers\n",
            "  copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib/apex/contrib/test/optimizers\n",
            "  creating build/lib/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/__init__.py -> build/lib/apex/contrib/test/xentropy\n",
            "  copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib/apex/contrib/test/xentropy\n",
            "  creating build/lib/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib/apex/contrib/test/focal_loss\n",
            "  copying apex/contrib/test/focal_loss/__init__.py -> build/lib/apex/contrib/test/focal_loss\n",
            "  creating build/lib/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib/apex/contrib/test/layer_norm\n",
            "  copying apex/contrib/test/layer_norm/__init__.py -> build/lib/apex/contrib/test/layer_norm\n",
            "  creating build/lib/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib/apex/contrib/test/clip_grad\n",
            "  copying apex/contrib/test/clip_grad/__init__.py -> build/lib/apex/contrib/test/clip_grad\n",
            "  creating build/lib/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib/apex/contrib/test/cudnn_gbn\n",
            "  copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib/apex/contrib/test/cudnn_gbn\n",
            "  creating build/lib/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/__init__.py -> build/lib/apex/contrib/test/transducer\n",
            "  copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib/apex/contrib/test/transducer\n",
            "  creating build/lib/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/test_fmha.py -> build/lib/apex/contrib/test/fmha\n",
            "  copying apex/contrib/test/fmha/__init__.py -> build/lib/apex/contrib/test/fmha\n",
            "  creating build/lib/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib/apex/contrib/test/index_mul_2d\n",
            "  copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib/apex/contrib/test/index_mul_2d\n",
            "  creating build/lib/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/__init__.py -> build/lib/apex/contrib/test/peer_memory\n",
            "  copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib/apex/contrib/test/peer_memory\n",
            "  creating build/lib/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/__init__.py -> build/lib/apex/contrib/test/group_norm\n",
            "  copying apex/contrib/test/group_norm/test_group_norm.py -> build/lib/apex/contrib/test/group_norm\n",
            "  creating build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/__init__.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib/apex/contrib/test/multihead_attn\n",
            "  creating build/lib/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/__init__.py -> build/lib/apex/contrib/test/bottleneck\n",
            "  copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib/apex/contrib/test/bottleneck\n",
            "  creating build/lib/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib/apex/contrib/test/conv_bias_relu\n",
            "  copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib/apex/contrib/test/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  copying build/lib/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  copying build/lib/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  copying build/lib/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
            "  copying build/lib/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  copying build/lib/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  copying build/lib/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  copying build/lib/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  copying build/lib/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  copying build/lib/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  copying build/lib/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  copying build/lib/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  copying build/lib/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  copying build/lib/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  copying build/lib/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing requirements to apex.egg-info/requires.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-34bgmwyq/.tmp-8c7rhime/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/cudnn_gbn/batch_norm.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/group_norm/group_norm.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/channel_swap.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/test/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/test/bottleneck/test_bottleneck_module.py'\n",
            "  adding 'apex/contrib/test/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/test/clip_grad/test_clip_grad.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/__init__.py'\n",
            "  adding 'apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py'\n",
            "  adding 'apex/contrib/test/fmha/__init__.py'\n",
            "  adding 'apex/contrib/test/fmha/test_fmha.py'\n",
            "  adding 'apex/contrib/test/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/test/focal_loss/test_focal_loss.py'\n",
            "  adding 'apex/contrib/test/group_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/group_norm/test_group_norm.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/test/index_mul_2d/test_index_mul_2d.py'\n",
            "  adding 'apex/contrib/test/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/test/layer_norm/test_fast_layer_norm.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_mha_fused_softmax.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn.py'\n",
            "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py'\n",
            "  adding 'apex/contrib/test/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_dist_adam.py'\n",
            "  adding 'apex/contrib/test/optimizers/test_distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/test/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py'\n",
            "  adding 'apex/contrib/test/transducer/__init__.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_joint.py'\n",
            "  adding 'apex/contrib/test/transducer/test_transducer_loss.py'\n",
            "  adding 'apex/contrib/test/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/test/xentropy/test_label_smoothing.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/_transducer_ref.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/_ucc_util.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=371405 sha256=19a1d1a05ec6ddfbdbb085d22ec596886f54ee6908425bef672a1dcb38c48784\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-30j6fb7o/wheels/83/7e/8e/84a535a8280d2de7a77a3fed6710fa86220721b5045c017df9\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir --no-build-isolation ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKSZwBG_uyzV",
        "outputId": "77bb168b-5223-4995-81b5-b366149d20a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.5/125.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.29.35-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.31.35 (from awscli)\n",
            "  Downloading botocore-1.31.35-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docutils<0.17,>=0.10 (from awscli)\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0 (from awscli)\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (6.0.1)\n",
            "Collecting colorama<0.4.5,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.31.35->awscli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.31.35->awscli) (2.8.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.31.35->awscli) (1.16.0)\n",
            "Installing collected packages: urllib3, rsa, jmespath, docutils, colorama, botocore, s3transfer, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "Successfully installed awscli-1.29.35 botocore-1.31.35 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.2 urllib3-1.25.4\n"
          ]
        }
      ],
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G07K5mE6JPF",
        "outputId": "0db26c78-6fba-4081-a5d1-c0ab2d7f5820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        }
      ],
      "source": [
        "%cd ditto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO49D4BeGlxb",
        "outputId": "c350898e-5ad9-4667-80c3-5f9298beba49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCMGQ1hPeBp7"
      },
      "source": [
        "Prepare datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uas6W3khe31K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4I4TqwpjNuD"
      },
      "outputs": [],
      "source": [
        "def prep_dataset(source_path,dest_path,label_column):\n",
        "  dataset = pd.read_csv(source_path)\n",
        "  entity_a_columns = [row for row in dataset if row.startswith(\"a_\")]\n",
        "  entity_b_columns = [row for row in dataset if row.startswith(\"b_\")]\n",
        "\n",
        "  text = \"\"\n",
        "\n",
        "  for index, row in dataset.iterrows():\n",
        "    for c in entity_a_columns:\n",
        "      text += \"COL \"\n",
        "      text += str(c)[2:] #remove prefix a_\n",
        "      text += \" VAL \"\n",
        "      text += str(row[c])\n",
        "      text += \" \"\n",
        "    text += \"\\t\"\n",
        "    for c in entity_b_columns:\n",
        "      text += \"COL \"\n",
        "      text += str(c)[2:] #remove prefix b_\n",
        "      text += \" VAL \"\n",
        "      text += str(row[c])\n",
        "      text += \" \"\n",
        "    text += \"\\t\"\n",
        "    text += str(row[label_column])\n",
        "    text += \"\\n\"\n",
        "\n",
        "  #export output\n",
        "  text_file = open(dest_path, \"w\")\n",
        "  n = text_file.write(text)\n",
        "  text_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH04qJakfT0y"
      },
      "outputs": [],
      "source": [
        "prep_dataset(source_path = \"/content/ditto/data/people/train.csv\",\n",
        "             dest_path = \"/content/ditto/data/people/train.txt\",\n",
        "             label_column = \"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRiLf71SpOo4"
      },
      "outputs": [],
      "source": [
        "prep_dataset(source_path = \"/content/ditto/data/people/valid.csv\",\n",
        "             dest_path = \"/content/ditto/data/people/valid.txt\",\n",
        "             label_column = \"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSdSqk5DeA8T"
      },
      "outputs": [],
      "source": [
        "prep_dataset(source_path = \"/content/ditto/data/people/test.csv\",\n",
        "             dest_path = \"/content/ditto/data/people/test.txt\",\n",
        "             label_column = \"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZeo7FSkPH5"
      },
      "source": [
        "Fine tune model with custom data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUFkZ3d9eBDE",
        "outputId": "004eb6d2-4049-4787-e5aa-55658c562331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 16:57:49.537323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.5859799385070801\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 1: dev_f1=0.4, f1=0.4347826086956522, best_f1=0.4347826086956522\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5583453178405762\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.4, f1=0.4347826086956522, best_f1=0.4347826086956522\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5709710121154785\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.4, f1=0.4347826086956522, best_f1=0.4347826086956522\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.7809371948242188\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.4, f1=0.4347826086956522, best_f1=0.4347826086956522\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.684744119644165\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4, f1=0.4347826086956522, best_f1=0.4347826086956522\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5418385863304138\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.4210526315789474, f1=0.4878048780487805, best_f1=0.4878048780487805\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4685671329498291\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.48, f1=0.15384615384615383, best_f1=0.15384615384615383\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5048485398292542\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6, f1=0.15384615384615383, best_f1=0.15384615384615383\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.7170647382736206\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6666666666666666, f1=0.7368421052631577, best_f1=0.7368421052631577\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31807541847229004\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5714285714285714, f1=0.588235294117647, best_f1=0.7368421052631577\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5429970622062683\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.625, f1=0.30769230769230765, best_f1=0.7368421052631577\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4409075379371643\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7000000000000001, f1=0.7368421052631577, best_f1=0.7368421052631577\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2769182324409485\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7000000000000001, f1=0.7368421052631577, best_f1=0.7368421052631577\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4419194459915161\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7777777777777777, f1=0.8000000000000002, best_f1=0.8000000000000002\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32509845495224\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8, f1=0.8181818181818182, best_f1=0.8181818181818182\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2501624822616577\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 16: dev_f1=0.8, f1=0.7826086956521738, best_f1=0.8181818181818182\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17509600520133972\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 17: dev_f1=0.8, f1=0.8181818181818182, best_f1=0.8181818181818182\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09094500541687012\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 18: dev_f1=0.8421052631578948, f1=0.8000000000000002, best_f1=0.8000000000000002\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08392412960529327\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 19: dev_f1=0.8, f1=0.8571428571428572, best_f1=0.8000000000000002\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1586892455816269\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 20: dev_f1=0.8421052631578948, f1=0.761904761904762, best_f1=0.8000000000000002\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028672143816947937\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 21: dev_f1=0.6666666666666666, f1=0.7058823529411764, best_f1=0.8000000000000002\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.7542822360992432\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 22: dev_f1=0.875, f1=0.7777777777777777, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2575245499610901\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 23: dev_f1=0.8421052631578948, f1=0.8000000000000002, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03022509068250656\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 24: dev_f1=0.8421052631578948, f1=0.8571428571428572, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.037638697773218155\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 25: dev_f1=0.8421052631578948, f1=0.8571428571428572, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18499597907066345\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 26: dev_f1=0.7777777777777777, f1=0.8000000000000002, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2070423811674118\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 27: dev_f1=0.8421052631578948, f1=0.9090909090909091, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015838101506233215\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 28: dev_f1=0.8421052631578948, f1=0.8571428571428572, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03182918205857277\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 29: dev_f1=0.823529411764706, f1=0.8421052631578948, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01556282676756382\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 30: dev_f1=0.8421052631578948, f1=0.8695652173913044, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.034512218087911606\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 31: dev_f1=0.823529411764706, f1=0.8421052631578948, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019142290577292442\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 32: dev_f1=0.8421052631578948, f1=0.9090909090909091, best_f1=0.7777777777777777\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21625378727912903\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 33: dev_f1=0.888888888888889, f1=0.8571428571428572, best_f1=0.8571428571428572\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02603057026863098\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 34: dev_f1=0.9333333333333333, f1=0.888888888888889, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01182577945291996\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 35: dev_f1=0.9333333333333333, f1=0.888888888888889, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031500041484832764\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 36: dev_f1=0.888888888888889, f1=0.8571428571428572, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02105143293738365\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 37: dev_f1=0.8421052631578948, f1=0.8695652173913044, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17306075990200043\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 38: dev_f1=0.8421052631578948, f1=0.8571428571428572, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2363666296005249\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 39: dev_f1=0.875, f1=0.8235294117647058, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003944122698158026\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 40: dev_f1=0.9333333333333333, f1=0.888888888888889, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05314432457089424\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 41: dev_f1=0.9333333333333333, f1=0.8421052631578948, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12522879242897034\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 42: dev_f1=0.888888888888889, f1=0.9523809523809523, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15656504034996033\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 43: dev_f1=0.888888888888889, f1=1.0, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17240075767040253\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 44: dev_f1=0.9411764705882353, f1=0.9473684210526316, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025837918743491173\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 45: dev_f1=1.0, f1=0.9473684210526316, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00612438702955842\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 46: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008342170156538486\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 47: dev_f1=1.0, f1=0.9473684210526316, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010840224102139473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 48: dev_f1=0.9411764705882353, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008960139937698841\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 49: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001926354249008\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 50: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002477891044691205\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 51: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011325596831738949\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 52: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014110264601185918\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 53: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007326528429985046\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 54: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008751081768423319\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 55: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014470701571553946\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 56: dev_f1=1.0, f1=1.0, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38499319553375244\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 57: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012790965847671032\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 58: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012264632619917393\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 59: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001099628396332264\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 60: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007368326769210398\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 61: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001727107330225408\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 62: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006647921982221305\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 63: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5735077857971191\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 64: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008363422239199281\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 65: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007278597331605852\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 66: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001524382154457271\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 67: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13816885650157928\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 68: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038385989610105753\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 69: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011258969316259027\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 70: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012199936900287867\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 71: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003649113466963172\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 72: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003923098091036081\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 73: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006907401839271188\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 74: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031997254118323326\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 75: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011504309950396419\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 76: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009697653586044908\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 77: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006517657311633229\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 78: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011973903747275472\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 79: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000590279814787209\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 80: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002178301103413105\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 81: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16698649525642395\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 82: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002196561312302947\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 83: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009070444502867758\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 84: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003449117997661233\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 85: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017871245509013534\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 86: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004538937006145716\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 87: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003639770671725273\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 88: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00807103794068098\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 89: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1135876476764679\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 90: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000596854486502707\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 91: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008751299465075135\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 92: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005351674742996693\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 93: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001023877295665443\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 94: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18369564414024353\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 95: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007480761269107461\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 96: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006478448049165308\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 97: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024265183601528406\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 98: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0035897514317184687\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 99: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010810432955622673\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 100: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task custom_people \\\n",
        "  --batch_size 16 \\\n",
        "  --max_len 128 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 100 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da drop_col #\\"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best result:\n",
        "\n",
        "\n",
        "```\n",
        "epoch 99: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
        "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
        "  warnings.warn(\"An input tensor was not cuda.\")\n",
        "step: 0, loss: 0.010810432955622673\n",
        "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
        "  warnings.warn(\"An input tensor was not cuda.\")\n",
        "epoch 100: dev_f1=1.0, f1=0.9523809523809523, best_f1=0.9473684210526316\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Parameters\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task custom_people \\\n",
        "  --batch_size 16 \\\n",
        "  --max_len 128 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 100 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da drop_col #\\\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "so3Vh62L8d-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnjfO4vF_Jq4",
        "outputId": "e9306ae7-fd36-477b-9fdb-ffdb930b982d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 17:01:22.680433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "32it [00:00, 51602.36it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 1.0\n",
            "real_f1 = 1.0\n",
            "49it [00:00, 139620.17it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task custom_people \\\n",
        "  --input_path /content/ditto/data/people/train.txt \\\n",
        "  --output_path output/people_output_train.txt \\\n",
        "  --lm roberta \\\n",
        "  --max_len 128 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35h5Y4HCfuEX",
        "outputId": "2738bc0c-6cd2-46b5-82fb-4a603e86f157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 17:01:43.001601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "32it [00:00, 61965.71it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 1.0\n",
            "real_f1 = 1.0\n",
            "32it [00:00, 66247.64it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task custom_people \\\n",
        "  --input_path /content/ditto/data/people/valid.txt \\\n",
        "  --output_path output/people_output_valid.txt \\\n",
        "  --lm roberta \\\n",
        "  --max_len 128 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhlyZmNGzYU9",
        "outputId": "deed4479-1215-497c-d0ac-10cf4edc1e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-27 17:02:02.447970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "32it [00:00, 58635.97it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 1.0\n",
            "real_f1 = 1.0\n",
            "36it [00:00, 136894.78it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task custom_people \\\n",
        "  --input_path /content/ditto/data/people/test.txt \\\n",
        "  --output_path output/people_output_test.txt \\\n",
        "  --lm roberta \\\n",
        "  --max_len 128 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9AezpXNg2Np"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuKpgKKr7rbc"
      },
      "source": [
        "Format Output as CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDwT1MIY-Yxe"
      },
      "outputs": [],
      "source": [
        "def format_output_as_csv(source_path, raw_results_path, dest_path):\n",
        "  df = pd.read_csv(source_path)\n",
        "  df_results = pd.read_json(raw_results_path,lines=True)\n",
        "\n",
        "  df[\"match\"] = df_results[\"match\"]\n",
        "  df[\"match_confidence\"] = df_results[\"match_confidence\"]\n",
        "\n",
        "  df.to_csv(dest_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDIecE0P-wP5"
      },
      "outputs": [],
      "source": [
        "format_output_as_csv(source_path = \"/content/ditto/data/people/train.csv\",\n",
        "  raw_results_path = \"/content/ditto/output/people_output_train.txt\",\n",
        "  dest_path = \"/content/ditto/output/people_output_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHfilGV-icNY"
      },
      "outputs": [],
      "source": [
        "format_output_as_csv(source_path = \"/content/ditto/data/people/valid.csv\",\n",
        "  raw_results_path = \"/content/ditto/output/people_output_valid.txt\",\n",
        "  dest_path = \"/content/ditto/output/people_output_valid.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj_Q0RKrhSEr"
      },
      "outputs": [],
      "source": [
        "format_output_as_csv(source_path = \"/content/ditto/data/people/test.csv\",\n",
        "  raw_results_path = \"/content/ditto/output/people_output_test.txt\",\n",
        "  dest_path = \"/content/ditto/output/people_output_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COReJWpS8ekO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e4G7nUd8enO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkUd1Epd8eqF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original code below (for reference only)"
      ],
      "metadata": {
        "id": "o0Sw4A2iyOeo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPZUJARUt_7U",
        "outputId": "af1275e2-4346-4c44-e434-0943f650682f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-26 18:20:42.317312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 3.41MB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 12.1MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 13.0MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 33.6MB/s]\n",
            "Downloading model.safetensors: 100% 499M/499M [00:03<00:00, 133MB/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.5665071606636047\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4607182443141937\n",
            "step: 20, loss: 0.5191353559494019\n",
            "step: 30, loss: 0.3480338454246521\n",
            "step: 40, loss: 0.10023485869169235\n",
            "step: 50, loss: 0.11979644745588303\n",
            "step: 60, loss: 0.14031493663787842\n",
            "step: 70, loss: 0.21377235651016235\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.09316705167293549\n",
            "step: 90, loss: 0.1288658231496811\n",
            "step: 100, loss: 0.5108204483985901\n",
            "step: 110, loss: 0.011245671659708023\n",
            "step: 120, loss: 0.28027427196502686\n",
            "step: 130, loss: 0.12761446833610535\n",
            "step: 140, loss: 0.11386328190565109\n",
            "step: 150, loss: 0.004554091487079859\n",
            "step: 160, loss: 0.044515348970890045\n",
            "step: 170, loss: 0.04838710278272629\n",
            "step: 180, loss: 0.030326761305332184\n",
            "step: 190, loss: 0.018941057845950127\n",
            "step: 200, loss: 0.002435960341244936\n",
            "step: 210, loss: 0.0016790891531854868\n",
            "step: 220, loss: 0.008898601867258549\n",
            "step: 230, loss: 0.03694805130362511\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 1: dev_f1=0.9633740288568259, f1=0.9684684684684683, best_f1=0.9684684684684683\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017085397616028786\n",
            "step: 10, loss: 0.0388229675590992\n",
            "step: 20, loss: 0.04796819016337395\n",
            "step: 30, loss: 0.02731555886566639\n",
            "step: 40, loss: 0.03726615756750107\n",
            "step: 50, loss: 0.0009206398390233517\n",
            "step: 60, loss: 0.07953061908483505\n",
            "step: 70, loss: 0.04343416541814804\n",
            "step: 80, loss: 0.003046383848413825\n",
            "step: 90, loss: 0.0024172833655029535\n",
            "step: 100, loss: 0.012481199577450752\n",
            "step: 110, loss: 0.06575514376163483\n",
            "step: 120, loss: 0.00027719189529307187\n",
            "step: 130, loss: 0.015098283998668194\n",
            "step: 140, loss: 0.004243411123752594\n",
            "step: 150, loss: 0.047488704323768616\n",
            "step: 160, loss: 0.02101028524339199\n",
            "step: 170, loss: 0.013146950863301754\n",
            "step: 180, loss: 0.1753014475107193\n",
            "step: 190, loss: 0.026631144806742668\n",
            "step: 200, loss: 0.16098012030124664\n",
            "step: 210, loss: 0.06694097816944122\n",
            "step: 220, loss: 0.02230885811150074\n",
            "step: 230, loss: 0.035564322024583817\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9832026875699889, f1=0.9808773903262092, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0023623439483344555\n",
            "step: 10, loss: 0.00445949099957943\n",
            "step: 20, loss: 0.004459935240447521\n",
            "step: 30, loss: 0.0643749088048935\n",
            "step: 40, loss: 0.26880306005477905\n",
            "step: 50, loss: 0.00781260710209608\n",
            "step: 60, loss: 0.003431829158216715\n",
            "step: 70, loss: 0.014699325896799564\n",
            "step: 80, loss: 0.010318956337869167\n",
            "step: 90, loss: 0.0006611186545342207\n",
            "step: 100, loss: 0.042196668684482574\n",
            "step: 110, loss: 0.0005930361221544445\n",
            "step: 120, loss: 0.0014794335002079606\n",
            "step: 130, loss: 0.11420216411352158\n",
            "step: 140, loss: 0.007285694126039743\n",
            "step: 150, loss: 0.01790408045053482\n",
            "step: 160, loss: 0.003070475533604622\n",
            "step: 170, loss: 0.05128210037946701\n",
            "step: 180, loss: 0.011683809570968151\n",
            "step: 190, loss: 0.003732551820576191\n",
            "step: 200, loss: 0.09986135363578796\n",
            "step: 210, loss: 0.01463789027184248\n",
            "step: 220, loss: 0.03566262125968933\n",
            "step: 230, loss: 0.006995482370257378\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9854096520763187, f1=0.9843400447427293, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006526425946503878\n",
            "step: 10, loss: 0.009221955202519894\n",
            "step: 20, loss: 0.041700560599565506\n",
            "step: 30, loss: 0.012419816106557846\n",
            "step: 40, loss: 0.058506958186626434\n",
            "step: 50, loss: 0.0005515769007615745\n",
            "step: 60, loss: 0.004139892756938934\n",
            "step: 70, loss: 0.00812512170523405\n",
            "step: 80, loss: 0.00039353370084427297\n",
            "step: 90, loss: 0.0009223025990650058\n",
            "step: 100, loss: 0.07307511568069458\n",
            "step: 110, loss: 0.002472362481057644\n",
            "step: 120, loss: 0.022167202085256577\n",
            "step: 130, loss: 0.024377867579460144\n",
            "step: 140, loss: 0.04735853895545006\n",
            "step: 150, loss: 0.00028708644094876945\n",
            "step: 160, loss: 0.026896372437477112\n",
            "step: 170, loss: 0.02876165136694908\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ditto/ditto/train_ditto.py\", line 89, in <module>\n",
            "    train(train_dataset,\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 201, in train\n",
            "    train_step(train_iter, model, optimizer, scheduler, hp)\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 137, in train_step\n",
            "    scaled_loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 128 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 20 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --da drop_col\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "## Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qxLFPNvcGgH",
        "outputId": "10190428-ddd6-4e68-ccde-a7e3474dcccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-26 18:28:43.957804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.8009777665138245\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ditto/ditto/train_ditto.py\", line 89, in <module>\n",
            "    train(train_dataset,\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 201, in train\n",
            "    train_step(train_iter, model, optimizer, scheduler, hp)\n",
            "  File \"/content/ditto/ditto/ditto_light/ditto.py\", line 123, in train_step\n",
            "    for i, batch in enumerate(train_iter):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/ditto/ditto/ditto_light/dataset.py\", line 80, in __getitem__\n",
            "    left, right = combined.split(' [SEP] ')\n",
            "ValueError: not enough values to unpack (expected 2, got 1)\n",
            "2023-08-26 18:28:57.338527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ditto/ditto/matcher.py\", line 314, in <module>\n",
            "    config, model = load_model(hp.task, hp.checkpoint_path,\n",
            "  File \"/content/ditto/ditto/matcher.py\", line 274, in load_model\n",
            "    raise ModelNotFoundError(checkpoint)\n",
            "ditto_light.exceptions.ModelNotFoundError: Model checkpoints/wdc_all_small/model.pt was not found\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the\n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task wdc_all_small \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 64 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 5 \\\n",
        "  --finetuning \\\n",
        "  --lm distilbert \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da drop_col\n",
        "\n",
        "# Step 2: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task wdc_all_small \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm distilbert \\\n",
        "  --max_len 64 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os8fsSZxklGJ",
        "outputId": "ba5b8baa-acdb-48d6-9d9f-e27bca954bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-26 18:38:36.641267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "step: 0, loss: 0.5504743456840515\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 1: dev_f1=0.358974358974359, f1=0.32558139534883723, best_f1=0.32558139534883723\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5762977004051208\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.38095238095238093, f1=0.5714285714285714, best_f1=0.5714285714285714\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26557838916778564\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6, f1=0.4210526315789473, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29831382632255554\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.35, f1=0.32558139534883723, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2194756418466568\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.4057971014492754, f1=0.3888888888888889, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18310202658176422\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.3888888888888889, f1=0.37837837837837834, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17487062513828278\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.3888888888888889, f1=0.4444444444444445, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10131863504648209\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.3448275862068965, f1=0.2553191489361702, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023612340912222862\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.31746031746031744, f1=0.24, best_f1=0.4210526315789473\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012957234866917133\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.31746031746031744, f1=0.24, best_f1=0.4210526315789473\n",
            "2023-08-26 18:39:36.487549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 57891.96it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7857142857142857\n",
            "real_f1 = 0.7857142857142857\n",
            "91it [00:00, 98600.27it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 64 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 10 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da drop_col #\\\n",
        "  #--balance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWCPmrDRG8VN",
        "outputId": "734baf4f-faef-404d-c63f-090adaed5e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-26 18:57:41.968853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 61820.81it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7857142857142857\n",
            "real_f1 = 0.7857142857142857\n",
            "5it [00:00, 6734.59it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_initialize.py:27: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_person.txt \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --max_len 64 \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oCmbXZ0Jon8"
      },
      "outputs": [],
      "source": [
        "#test data, copy to input folder\n",
        "\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Thomas Anderson COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Ong Ah Seng COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Jeffrey Ong COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 52 \tCOL Beer_Name VAL Ong Ah Seng Jeffrey COL Brew_Factory_Name VAL Singapore COL Style VAL Male COL ABV VAL 50 \t0\n",
        "COL Beer_Name VAL Bulleit Bourbon Barrel Aged G'Knight COL Brew_Factory_Name VAL Oskar Blues Grill & Brew COL Style VAL American Amber / Red Ale COL ABV VAL 8.70 % \tCOL Beer_Name VAL Figure Eight Bourbon Barrel Aged Jumbo Love COL Brew_Factory_Name VAL Figure Eight Brewing COL Style VAL Barley Wine COL ABV VAL - \t0\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}